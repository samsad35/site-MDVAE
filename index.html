<!DOCTYPE HTML>
<html>
	<head>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-109046767-2');
		</script>
		<link rel="icon" type="image/png" href="demos/onglet.png" />

		<!-- CHANGE HERE -->
		<title>A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning</title>
		<!-- ----------- -->
		<script src="https://cdn.jsdelivr.net/npm/before-after-slider@1.0.0/dist/slider.bundle.js"></script>


		<script
		  defer
		  src="https://unpkg.com/img-comparison-slider@7/dist/index.js"
		></script>
		<link
		  rel="stylesheet"
		  href="https://unpkg.com/img-comparison-slider@7/dist/styles.css"
		/>

		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<link href="http://fonts.googleapis.com/css?family=Source+Sans+Pro:300,300italic,600|Source+Code+Pro" rel="stylesheet" />
		<script src="http://code.jquery.com/jquery-1.10.1.min.js"></script>
		<script src="js/jquery.dropotron.min.js"></script>
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.css" integrity="sha384-D+9gmBxUQogRLqvARvNLmA9hS2x//eK1FhVb9PiU86gmcrBrJAQT8okdJ4LMp2uv" crossorigin="anonymous">

		<!-- The loading of KaTeX is deferred to speed up page rendering -->
		<script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/katex.min.js" integrity="sha384-483A6DwYfKeDa0Q52fJmxFXkcPCFfnXMoXblOkJ4JcA8zATN6Tm78UNL72AKk+0O" crossorigin="anonymous"></script>

		<!-- To automatically render math in text elements, include the auto-render extension: -->
		<script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.0-rc.1/dist/contrib/auto-render.min.js" integrity="sha384-yACMu8JWxKzSp/C1YV86pzGiQ/l1YUfE8oPuahJQxzehAjEt2GiQuy/BIvl9KyeF" crossorigin="anonymous"
        onload="renderMathInElement(document.body);"></script>
		<script src="js/skel.min.js">
		{
			prefix: 'css/style',
			preloadStyleSheets: true,
			resetCSS: true,
			boxModel: 'border',
			grid: { gutters: 30 },
			breakpoints: {
				wide: { range: '1200-', containers: 1140, grid: { gutters: 50 } },
				narrow: { range: '481-1199', containers: 960 },
				mobile: { range: '-480', containers: 'fluid', lockViewport: true, grid: { collapse: true } }
			}
		}
		</script>

		<style>
			table tr th, table tr td {
			text-align: center;
			vertical-align: middle;
			border: 0px solid white;
			border-collapse: collapse;
			}
		</style>

		<style type="text/css">a {text-decoration: none}</style>

	</head>

	<body>

		<div id="site_content">
		<div class="container">

		<!-- Features -->
		<div class="row">
			<section class="12u">

				<h2 style="text-align: center;">
					A Multimodal Dynamical Variational Autoencoder for Audiovisual Speech Representation Learning
				</h2>

				<p style="text-align: center;">
					Samir Sadok<sup>1</sup>&emsp;&emsp; Simon Leglaive<sup>1</sup>&emsp;&emsp; Laurent Girin<sup>2</sup>&emsp;&emsp; Xavier Alameda-Pineda<sup>3</sup>&emsp;&emsp; Renaud Séguier<sup>1</sup>&emsp;&emsp;
				</p>

				<p style="text-align: center; line-height: 100%;">
					<sup>1</sup>CentraleSupélec, IETR UMR CNRS 6164, France &emsp;&emsp; <br>
					<sup>2</sup>Univ. Grenoble Alpes, CNRS, Grenoble-INP, GIPSA-lab, France  &emsp;&emsp;<br>
					<sup>3</sup>Inria, Univ. Grenoble Alpes, CNRS, LJK, France<br>
				</p>
				<!--
				<p style="text-align: center;">
					Inproceedings
				</p> 
				-->

				<p style="text-align: center;">
					 <a href="https://arxiv.org/MDVAE" target="_blank" rel="noopener">Article</a> |
					<!-- <a href="#audio">Audio examples</a> |  -->
<!--					 <a href="documents/presentation_icassp2020.pdf">Slides</a> |  -->
					<a href="https://github.com/samsad35/code-mdvae" target="_blank" rel="noopener">Code</a>
					<!-- <a href="https://hal.archives-ouvertes.fr/hal-03603791v1/bibtex" target="_blank" rel="noopener">Bibtex</a> | -->
					<!-- <a href="#acknowledgement">Acknowledgement</a> -->
				</p>

				<!-- Abstract -->
				<strong><span style="font-size: large;"><a name="abstract"></a>Abstract</span></strong>
				<hr>
				
				
<!--				<div style="width:100%; text-align: center;">-->
<!--					<img src="demos/MDVAE/graph.png" align="right" width="600" title="MDVAE’s graphical model in compact form"> -->
<!--				</div>-->
				<p style = "text-align: justify; vertical-align: top;"> High-dimensional data such as natural images or speech signals exhibit some form of regularity, preventing their dimensions from varying independently. This suggests that there exists a lower dimensional latent representation from which the high-dimensional observed data were generated. Uncovering the hidden explanatory features of complex data is the goal of representation learning, and deep latent variable generative models have emerged as promising unsupervised approaches. In particular, the variational autoencoder (VAE) which is equipped with both a generative and inference model allows for the analysis, transformation, and generation of various types of data. Over the past few years, the VAE has been extended to deal with data that are either multimodal \textit{or} dynamical (i.e., sequential).
									In this paper, we present a multimodal \textit{and} dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage.
										Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture. </p>

				<div style="width:100%; text-align: center">
					<table>
						<td>
							<img src="demos/MDVAE/VQ-MDVAE.png" alt="VQ-MDVAE"  width="880" />
						</td>

						<td>
							<img src="demos/MDVAE/notations.PNG" alt="F1 subspace"  width="250" />
						</td>
					</table>
				</div>
			<!-- VISUALISATION OF THE LEARNED LATENT SUBSPACES -->
			<span style="font-size: large;"><a name="latentspace"></a><strong>What is encoded in the latent spaces of the MDVAE?</strong></span>
			<hr>
			
			We will present qualitative results obtained by reconstructing an audiovisual speech sequence using some of the latent variables from another sequence.
			<br>
			<span style="font-size: medium; color:#757470;"><a name="Visual"></a><strong>For the visual modality:</strong></span>
			

			<div style="width:100%; text-align: justify; border: 1px solid black;">
			<table style="width:100%; text-align: center;">
					<td>
					<p> We transfer <span style="color:red">\(\mathbf{z}^{(v)}\) </span> from the central sequence in red to the surrounding sequences.</p>
						<img src="demos/Gifs/z_visual.gif" alt="z_v"  width="300" />
					<p> Only head and eye movements are transfered.</p>
					</td>
					
					<td>
					<p> We transfer <span style="color:green">\(\mathbf{z}^{(av)}\) </span> from the central sequence in red to the surrounding sequences.</p>
						<img src="demos/Gifs/z_av.gif" alt="z_v"  width="300" />
						<p> Only lip movements are transfered.</p>
					</td>
					
					<td>
					<p> We transfer <span style="color:red">\(\mathbf{z}^{(v)}\) </span> and <span style="color:green">\(\mathbf{z}^{(av)}\) </span> from the central sequence in red to the surrounding sequences.</p>
						<img src="demos/Gifs/gif-4.gif" alt="z_v"  width="300" />
						<p> All dynamical factors are transfered.</p>
					</td>
			</table>
			</div>
			<br>
			<div style="border: 1px solid black;">
			<style type="text/css">
			  img-comparison-slider {
				--divider-width: 3px;
				--divider-color: #48C9B0;
				--default-handle-width: 80px;
				--default-handle-opacity: 1.0;
				--divider-shadow: 0px 0px 5px rgba(0, 0, 1, 0.5);
				--default-handle-color: #48C9B0;
			  }
			</style>
			<center>
      			<b id="number" style="color:#5E77E7;">Example n° 1</b><br>
      			<button id="back" onclick="back()" style="border-radius: 10px; width: 300px;}">Back</button>
      			<button id="next" onclick="next()" style="border-radius: 10px; width: 300px;}">Next</button>
			</center>
			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
				<td>
					<p> Two original visual sequences. (Scroll left) the visual sequence of a man; (Scroll right) the visual sequence of a woman;  </p>
					<img-comparison-slider>
						<img id="original-2" class="source" slot="first" src="demos/sliders/images/exemple-1/2.png" width="100%"/>
						<img id="original-1" class="source" slot="second" src="demos/sliders/images/exemple-1/1.png" width="100%" />
					</img-comparison-slider>
				</td>
				<td>
			</table>
			</div>
			<script>
				backButton = document.getElementById("back");
				nextButton = document.getElementById("next");
				number = document.getElementById("number");
				var count = 1;
				if (count == 1){
					backButton.disabled = true;
				}

				function next(){
					count = count + 1;
					if (count != 1){
						backButton.disabled = false;
					}
					if (count == 3){
						nextButton.disabled = true;
					}
					number.innerHTML = "Example n° " + count;
					document.getElementById('original-2').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-v').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-av').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-av-v').src="demos/sliders/images/exemple-"+count+"/2.png"


					document.getElementById('zv').src="demos/sliders/images/exemple-"+count+"/zv.png"
					document.getElementById('zv-2').src="demos/sliders/images/exemple-"+count+"/zv-2.png"

					document.getElementById('zav').src="demos/sliders/images/exemple-"+count+"/zav.png"
					document.getElementById('zav-2').src="demos/sliders/images/exemple-"+count+"/zav-2.png"

					document.getElementById('zav-zv').src="demos/sliders/images/exemple-"+count+"/zav-zv.png"
					document.getElementById('zav-zv-2').src="demos/sliders/images/exemple-"+count+"/zav-zv-2.png"


					document.getElementById('original-1').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-v').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-av').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-av-v').src="demos/sliders/images/exemple-"+count+"/1.png"

					}

				function back(){
					count = count - 1;
					if (count == 1){
						backButton.disabled = true;
					}
					if (count < 3){
						nextButton.disabled = false;
					}
					number.innerHTML = "Example n° " + count;
					document.getElementById('original-2').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-v').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-av').src="demos/sliders/images/exemple-"+count+"/2.png"
					document.getElementById('original-2-av-v').src="demos/sliders/images/exemple-"+count+"/2.png"

					document.getElementById('zv').src="demos/sliders/images/exemple-"+count+"/zv.png"
					document.getElementById('zv-2').src="demos/sliders/images/exemple-"+count+"/zv-2.png"

					document.getElementById('zav').src="demos/sliders/images/exemple-"+count+"/zav.png"
					document.getElementById('zav-2').src="demos/sliders/images/exemple-"+count+"/zav-2.png"

					document.getElementById('zav-zv').src="demos/sliders/images/exemple-"+count+"/zav-zv.png"
					document.getElementById('zav-zv-2').src="demos/sliders/images/exemple-"+count+"/zav-zv-2.png"

					document.getElementById('original-1').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-v').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-av').src="demos/sliders/images/exemple-"+count+"/1.png"
					document.getElementById('original-1-av-v').src="demos/sliders/images/exemple-"+count+"/1.png"
					}
			</script>

			<style>
				img.target {
					filter: brightness(70%);
				}
			</style>

			<style>
				img.source {
				   filter: brightness(110%);
				}
			</style>
			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
				<tr>
					<td>
						<p> We transfer <span style="color:red">\(\mathbf{z}^{(v)}\) </span> from the background sequence to the foreground sequence.  </p>
						<img-comparison-slider value="10">
							<img id="zv" class="source" slot="first" src="demos/sliders/images/exemple-1/zv.png" width="100%"/>
							<img id="original-1-v" class="target" slot="second" src="demos/sliders/images/exemple-1/1.png" width="100%" />
						</img-comparison-slider>
					</td>
					<td>
						<p> We transfer <span style="color:green">\(\mathbf{z}^{(av)}\) </span> from the background sequence to the foreground sequence.  </p>
						<img-comparison-slider value="10">
							<img id="zav" class="source" slot="first" src="demos/sliders/images/exemple-1/zav.png" width="100%"/>
							<img id="original-1-av" class="target" slot="second" src="demos/sliders/images/exemple-1/1.png" width="100%" />
						</img-comparison-slider>
					</td>
				</tr>

				<tr>
					<td>
						<img-comparison-slider value="10">
							<img id="zv-2" class="source" slot="first" src="demos/sliders/images/exemple-1/zv-2.png" width="100%"/>
							<img id="original-2-v" class="target" slot="second" src="demos/sliders/images/exemple-1/2.png" width="100%" />
						</img-comparison-slider>
					</td>
					<td>
						<img-comparison-slider value="10">
							<img id="zav-2" class="source" slot="first" src="demos/sliders/images/exemple-1/zav-2.png" width="100%"/>
							<img id="original-2-av" class="target" slot="second" src="demos/sliders/images/exemple-1/2.png" width="100%" />
						</img-comparison-slider>
					</td>
				</tr>
			</table>
			</div>

				<div style="width:100%; text-align: center;">
					<table style="width:100%; text-align: center;">
				<tr>
					<td>
						<p> We transfer <span style="color:red">\(\mathbf{z}^{(v)}\) </span> and <span style="color:green">\(\mathbf{z}^{(av)}\) </span> from the background sequence to the foreground sequence. </p>
						<img-comparison-slider value="10">
							<img id="zav-zv" class="source" slot="first" src="demos/sliders/images/exemple-1/zav-zv.png" width="100%"/>
							<img id="original-1-av-v" class="target" slot="second" src="demos/sliders/images/exemple-1/1.png" width="100%" />
						</img-comparison-slider>
					</td>
				</tr>

				<tr>
					<td>
						<img-comparison-slider value="10">
							<img id="zav-zv-2" class="source" slot="first" src="demos/sliders/images/exemple-1/zav-zv-2.png" width="100%"/>
							<img id="original-2-av-v" class="target" slot="second" src="demos/sliders/images/exemple-1/2.png" width="100%" />
						</img-comparison-slider>
					</td>
				</tr>
			</table>
				</div>

			</div>
			<br>

			<div style="width:100%; text-align: center; border: 1px solid black;">
				In the annimation below, we transfer <span style="color:green">\(\mathbf{z}^{(av)}\) </span> and <span style="color:red">\(\mathbf{z}^{(v)}\) </span> from the central sequence in red to the surrounding sequences. The identity and global emotional state are preserved because <span style="color:green">\(\mathbf{w}\)</span> is unaltered.
				<video controls autoplay  muted preload="auto" width="1000" height="700">
				  <source src="demos/Video/video-13.mp4" type="video/mp4">
				  <source src="demos/Video/video-13.ogg" type="video/ogg">
				  Your browser does not support the video tag.
				</video>
			</div>




			<br>
			<br>
			<span style="font-size: strong; color:#757470;"><a name="Audio"></a><strong>For the audio modality:</strong></span>
			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
					<td>
						<p> <span style="color:blue">\(\mathbf{z}^{(a)}\) </span> and <span style="color:green">\(\mathbf{z}^{(av)}\) </span> are fixed, <span style="color:green">\(\mathbf{w}\)</span> varies.</p>
						<img src="demos/Gifs/vary_w.gif" alt="audio-w"  width="350" />
						<p> <span style="color:green">\(\mathbf{w}\)</span> seems to encode the speaker's audio identity. </p>
					</td>
					
					<td>
						<p> <span style="color:green">\(\mathbf{w}\)</span>  and <span style="color:green">\(\mathbf{z}^{(av)}\) </span> are fixed, <span style="color:blue">\(\mathbf{z}^{(a)}\) </span> varies.</p>
						<img src="demos/Gifs/vary_z_a.gif" alt="audio-za"  width="350" />
						<p> <span style="color:green">\(\mathbf{z}^{(v)}\) </span> encodes the high-frequency phonemic content.</p>
					</td>
					
					<td>
					<p> <span style="color:blue">\(\mathbf{z}^{(a)}\) </span> and <span style="color:green">\(\mathbf{w}\)</span> are fixed, <span style="color:green">\(\mathbf{z}^{(av)}\) </span> varies.</p>
						<img src="demos/Gifs/vary_z_ds.gif" alt="audio-zav"  width="350" />
						<p> <span style="color:green">\(\mathbf{z}^{(av)}\) </span> encodes the low-frequency phonemic content.</p>
					</td>
			</table>
			</div>

			<script>
				var za  = 1;
				var zav  = 1;
        		function myFunctionZa() {
				  za = Math.floor(Math.random()*100);
				  document.getElementById('za1').src="demos/generation/audio/zaudio-"+za+".png"
				  document.getElementById('za2').src="demos/generation/audio-2/zaudio-"+za+".png"
				}

				function myFunctionZav() {
				  zav = Math.floor(Math.random() * 100);
				  document.getElementById('zav1').src="demos/generation/audio/zds-"+zav+".png"
				  document.getElementById('zav2').src="demos/generation/audio-2/zds-"+zav+".png"
				}
    		</script>




			<!-- Interpolation in the static audiovisual latent space -->
			<span style="font-size: large;"><a name="latentspace"></a><strong>Interpolation of the static audiovisual latent variable <span style="color:green">\(\mathbf{w}\)</span></strong></span>
			<hr>

			
			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
					<td>
						<video controls autoplay  muted loop preload="auto" width="350">
						  <source src="demos/Video/identity_interpolation.mp4" type="video/mp4">
						  <source src="demos/Video/identity_interpolation.ogg" type="video/ogg">
						  Your browser does not support the video tag.
						</video>
						<p> Same emotion, different identities.</p>
					</td>
					<td>
						<video controls autoplay  muted loop preload="auto" width="350">
						  <source src="demos/Video/emotions_interpolation.mp4" type="video/mp4">
						  <source src="demos/Video/emotions_interpolation.ogg" type="video/ogg">
						  Your browser does not support the video tag.
						</video>
					<p>Same identity, different emotions. </p>
					</td>
			</table>
			</div>



			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
				<td>
					<p> Scroll right to change the identity in a smooth way, keeping the emotion;  </p>
					<img-comparison-slider value="10" hover=false>
						<img slot="second" src="demos/sliders/images/interpolation/identity-before.png" width="100%"/>
						<img slot="first" src="demos/sliders/images/interpolation/identity-after.png" width="100%" />
					</img-comparison-slider>
				</td>
				<td>
					<p> Scroll right to change the emotion in a smooth way, keeping the identity;  </p>
					<img-comparison-slider value="10" hover=false>
						<img slot="second" src="demos/sliders/images/interpolation/emotion-before.png" width="100%"/>
						<img slot="first" src="demos/sliders/images/interpolation/emotion-after.png" width="100%" />
					</img-comparison-slider>
				</td>
			</table>
			</div>

			<!-- Interpolation in the static audiovisual latent space -->
			<span style="font-size: large;"><a name="latentspacew"></a><strong>VISUALIZATION OF THE LATENT SPACES</strong></span>


			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
				<td>
					<div style="width:100%; text-align: center;">
<!--					<a href="https://chart-studio.plotly.com/~sam-sad/27/#plot">For a better view</a>-->
					<iframe src="https://plotly.com/~sam-sad/27/" frameborder="0"
							width="600" height="600" allowfullscreen="true"
							mozallowfullscreen="true" webkitallowfullscreen="true">
					</iframe>
					</div>
				</td>
				<td>
					<div style="width:100%; text-align: center;">
<!--					<a href="https://chart-studio.plotly.com/~sam-sad/23/#plot">For a better view</a>-->
					<iframe src="https://plotly.com/~sam-sad/23/" frameborder="0"
							width="600" height="600" allowfullscreen="true"
							mozallowfullscreen="true" webkitallowfullscreen="true">
					</iframe>
					</div>
				</td>
			</table>
			</div>

			<!-- Interpolation in the static audiovisual latent space -->
			<span style="font-size: large;"><a name="landmark+visual"></a><strong>conditioned audiovisual speech generation</strong></span>
				<p>We illustrate the ability of the MDVAE model to generate speech spectrograms with a qualitative example.  Figures belows show spectrograms generated by sampling according to the prior distribution of <span style="color:green">\(\mathbf{z}^{(av)}\) </span> and <span style="color:blue">\(\mathbf{z}^{(a)}\) </span>, conditioned on the first thirty frames. These first thirty frames (with a duration of 1s) are obtained through analysis-resynthesis. After 1s, we switch the MDVAE to pure generation mode. We can see that the spectrograms generated with the prior distribution of <span style="color:green">\(\mathbf{z}^{(av)}\) </span> exhibit a harmonic structure.</p>
			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
					<td>
						<p>  Example of speech power spectrogram reconstructed (0-1 s) and generated (1-2.6 s) by a MDVAE model.</p>
						<p><button onclick="myFunctionZa()"> Sampling according to the Gaussian distribution prior of <span style="color:blue">\(\mathbf{z}^{(a)}\) </span></button></p>
						<img id="za1" src="demos/generation/audio/original.png" alt="audio-w"  width="400" />
						<img id="za2" src="demos/generation/audio-2/original.png" alt="audio-w"  width="400" />
					</td>
					<td>
						<p>  Example of speech power spectrogram reconstructed (0-1 s) and generated (1-2.6 s) by a MDVAE model.</p>
						<p><button onclick="myFunctionZav()"> Sampling according to the Gaussian distribution prior of <span style="color:green">\(\mathbf{z}^{(av)}\) </span></button></p>
						<img id="zav1" src="demos/generation/audio/original.png" alt="audio-w"  width="400" />
						<img id="zav2" src="demos/generation/audio-2/original.png" alt="audio-w"  width="400" />
					</td>
			</table>
			</div>

				<p>Similarly, we illustrate the ability of the MDVAE model to generate visual frames with a qualitative example. Figure below illustrate this by sampling following the Gaussian prior of <span style="color:red">\(\mathbf{z}^{(v)}\) </span> and <span style="color:green">\(\mathbf{z}^{(av)}\) </span>, conditioned on the analyzed-resynthesized first frame of the sequence. This strategy of conditioning to the first frame, allows the generation mode to have temporally coherent sequences. The first lightened lines correspond to the original visual sequences, and the other lines are three different generations. As expected, we have a smooth transition from the analyzed-resynthesized frame to the generated one.</p>

			<div style="width:100%; text-align: center;">
			<table style="width:100%; text-align: center;">
					<td>
						<p>  Original sequence.</p>
						<img src="demos/generation/visual/0.png" alt="audio-w"  width="400" />
						<p>  Generated sequence.</p>
						<img id="zv1" src="demos/generation/visual/0.png" alt="tt"  width="400" />
						<img id="zav1_visual" src="demos/generation/visual/0.png" alt="tt"  width="400" />
					</td>
					<td>
						<p>  Original sequence.</p>
						<img id="original" src="demos/generation/visual-2/0.png" alt="audio-w"  width="400" />
						<p>  Generated sequence.</p>
						<img id="zv2" src="demos/generation/visual-2/0.png" alt="tt"  width="400" />
						<img id="zav2_visual" src="demos/generation/visual-2/0.png" alt="tt"  width="400" />
					</td>
					<td>
						<p><button onclick="myFunctionZav_()"> Sampling according to the Gaussian distribution prior of <span style="color:green">\(\mathbf{z}^{(av)}\) </span></button></p>
						<p><button onclick="myFunctionZv()"> Sampling according to the Gaussian distribution prior of <span style="color:red">\(\mathbf{z}^{(v)}\) </span></button></p>
					</td>
			</table>
			</div>

				<script>
				var zv  = 1;
				var zav  = 1;
        		function myFunctionZav_() {
				  zv = Math.floor(Math.random()*100) + 1;
				  document.getElementById('zv1').src="demos/generation/visual/"+zv+".png"
				  document.getElementById('zv2').src="demos/generation/visual-2/"+zv+".png"

				  document.getElementById('zav1_visual').src="demos/generation/visual/0.png"
				  document.getElementById('zav2_visual').src="demos/generation/visual-2/0.png"
				}

				function myFunctionZv() {
				  zav = Math.floor(Math.random() * 100)+1;
				  document.getElementById('zav1_visual').src="demos/generation/visual/"+zav+"_.png"
				  document.getElementById('zav2_visual').src="demos/generation/visual-2/"+zav+"_.png"

				  document.getElementById('zv1').src="demos/generation/visual/0.png"
				  document.getElementById('zv2').src="demos/generation/visual-2/0.png"
				}
    		</script>
<!--			<div style="width:100%; text-align: justify;">-->
<!--			<table style="width:100%; text-align: justify;">-->

<!--				<td>-->
<!--					<p>We trained VQ-MDVAE on the MEAD database, but replacing the audio modality by the lip landmarks only.</p>-->
<!--					<div style="width:100%; text-align: center; border: 1px solid black;" class="video">-->
<!--						<video controls  muted loop preload="auto" width="500" height="500">-->
<!--						  <source src="demos/Video/landmark-visual.mp4" type="video/mp4">-->
<!--						  <source src="demos/Video/landmark-visual.ogg" type="video/ogg">-->
<!--						  Your browser does not support the video tag.-->
<!--						</video>-->
<!--					</div>-->
<!--				</td>-->
<!--				<td>-->
<!--					<p>We trained VQ-MDVAE on the MEAD database, but replacing the audio modality by the left view.</p>-->
<!--					<div style="width:100%; text-align: center; border: 1px solid black;" class="video">-->
<!--						<video controls  muted loop preload="auto" width="500" height="500" >-->
<!--						  <source src="demos/Video/left+front.mp4" type="video/mp4">-->
<!--						  <source src="demos/Video/left+front.ogg" type="video/ogg">-->
<!--						  Your browser does not support the video tag.-->
<!--						</video>-->
<!--					</div>-->
<!--				</td>-->
<!--			</table>-->
<!--			</div>-->

<!--				<script>-->
<!--					var figure = $(".video").hover( hoverVideo, hideVideo );-->

<!--						function hoverVideo(e) {-->
<!--							$('video', this).get(0).play();-->
<!--						}-->

<!--						function hideVideo(e) {-->
<!--							$('video', this).get(0).pause();-->
<!--						}-->
<!--				</script>-->

		</div>
	</div>
</div>

</body>
</html>
